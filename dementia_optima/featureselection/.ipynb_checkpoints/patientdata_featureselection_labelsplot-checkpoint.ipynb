{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 655)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(130,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(130,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(130,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  0., 59.],\n",
       "       [ 0.,  1.,  0., ..., nan, nan, nan],\n",
       "       [ 0.,  1.,  0., ..., nan, nan, nan],\n",
       "       [ 0.,  1.,  1., ..., nan, nan, nan],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0., 66.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2, 5, 3, 4, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2, 5, 3, 4, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Normal', 'Fast_MiS', 'Normal_MiS', 'Fast', 'Slow'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load patient data\n",
    "import pickle\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "folder_path = '../../../datalcdem/data/optima/dementia_18July/class_mild_mod_sev/'\n",
    "\n",
    "d = pickle.load(open(folder_path + 'patient_data.pkl', 'rb'))\n",
    "\n",
    "patient_data =  Bunch(data=d['data'], target=d['target'], target_name=d['target_names'], target_real=d['target_real'])\n",
    "\n",
    "data_p = patient_data.data\n",
    "target_p = patient_data.target\n",
    "target_real_p = patient_data.target_real\n",
    "target_names_p = patient_data.target_name\n",
    "display(data_p.shape, target_p.shape, target_real_p.shape, target_names_p.shape, data_p[0:5], target_p[0:5], target_real_p[0:5], target_names_p[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARLElEQVR4nO3df6zddX3H8edrBQcDtCp3pKNgEVHDTCzursNhnKK4CiqYkAWmiAlL3RSDPzYHZj80mwsuCpq4mVVh1sn4EVBxoE6CNYQNwVssP0pxMCwRrPT6o4NGhyu898f5Nl5vb3tP7z3nnn7o85GcnO/38/1+z/f9bdMXHz7n+z2fVBWSpPb8yqgLkCTNjQEuSY0ywCWpUQa4JDXKAJekRhngktQoA1xNSbIhyStGXYe0NzDAtVdJsinJq6e1vTXJzQBV9ZtV9Y1ZPmNZkkqy3xBLlUbOAJf2kP9h0N7CAFdTpvbQk6xIMpHk0SSPJLmo2+2m7n1rkm1JXprkV5L8RZIHk2xJ8tkkz5jyuW/ptv0oyV9OO88Hklyd5HNJHgXe2p37liRbk2xO8okkT5vyeZXk7UnuS/JYkr9JcnSS/+zqvWrq/tJcGOBq2ceBj1fV04Gjgau69pd374ur6uCqugV4a/d6JfBc4GDgEwBJjgX+EXgTsAR4BnD4tHOdClwNLAYuA54A3g0cCrwUeBXw9mnH/D7wW8DxwPuA1cCbgSOAFwFnzuPaJQNce6Uvdj3brUm20gvXmfwf8Lwkh1bVtqr65m4+803ARVX1QFVtAy4AzuiGQ04H/q2qbq6qnwN/BUz/kaBbquqLVfVkVf2sqtZV1TerantVbQL+Cfi9acf8fVU9WlUbgLuBr3Xn/x/gK8Bx/f+RSDszwLU3Oq2qFu94sXPPdodzgOcD9yb5VpLX7eYzfwN4cMr6g8B+wGHdtu/t2FBVPwV+NO34701dSfL8JNcl+UE3rPJ39HrjUz0yZflnM6wfvJt6pVkZ4GpWVd1XVWcCvw58GLg6yUHs3HsG+D7wnCnrRwLb6YXqZmDpjg1JDgSePf1009Y/CdwLHNMN4bwfyNyvRtpzBrialeTNScaq6klga9f8JDDZvT93yu6XA+9OclSSg+n1mK+squ30xrZfn+R3uy8WP8DsYXwI8CiwLckLgT8Z1HVJ/TLA1bKVwIYk2+h9oXlGNz79U+BDwH904+jHA5cC/0LvDpXvAv8LvBOgG6N+J3AFvd74NmAL8Phuzv2nwB8CjwGfAq4c/OVJuxcndJB+WddD30pveOS7o65H2hV74BKQ5PVJfq0bQ/8IcBewabRVSbtngEs9p9L7ovP7wDH0hmP831Pt1RxCkaRG2QOXpEYt6I/yHHroobVs2bKFPKUkNW/dunU/rKqx6e0LGuDLli1jYmJiIU8pSc1L8uBM7Q6hSFKjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEb1HeBJFiX5dpLruvWjktya5P4kVzq/nyQtrD3pgZ8HbJyy/mHg4qp6HvATerOjSJIWSF8BnmQpcArw6W49wIn0fggfYA1w2jAKlCTNrN8nMT9Gb1btQ7r1ZwNbu9lMAB5i51m8AUiyClgFcOSRR869Ukmag2XnXz/qEgDYdOEpA//MWXvg3USxW6pq3VxOUFWrq2q8qsbHxnZ6lF+SNEf99MBPAN6Q5GTgAODp9KavWpxkv64XvhR4eHhlSpKmm7UHXlUXVNXSqloGnAF8vareBKwFTu92Oxu4dmhVSpJ2Mp/7wP8ceE+S++mNiV8ymJIkSf3Yo5+TrapvAN/olh8AVgy+JElSP3wSU5IaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUqH4mNT4gyW1J7kiyIckHu/bPJPlukvXda/nwy5Uk7dDPjDyPAydW1bYk+wM3J/lKt+3Pqurq4ZUnSdqVWQO8qgrY1q3u371qmEVJkmbX1xh4kkVJ1gNbgBuq6tZu04eS3Jnk4iS/OrQqJUk76SvAq+qJqloOLAVWJHkRcAHwQuC3gWfRm6V+J0lWJZlIMjE5OTmgsiVJe3QXSlVtBdYCK6tqc/U8Dvwzu5ihvqpWV9V4VY2PjY3Nv2JJEtDfXShjSRZ3ywcCJwH3JlnStQU4Dbh7mIVKkn5ZP3ehLAHWJFlEL/Cvqqrrknw9yRgQYD3wx0OsU5I0TT93odwJHDdD+4lDqUiS1BefxJSkRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RG9TMn5gFJbktyR5INST7YtR+V5NYk9ye5MsnThl+uJGmHfnrgjwMnVtWLgeXAyiTHAx8GLq6q5wE/Ac4ZXpmSpOlmDfDq2dat7t+9CjgRuLprX0NvZnpJ0gLpaww8yaIk64EtwA3AfwNbq2p7t8tDwOG7OHZVkokkE5OTk4OoWZJEnwFeVU9U1XJgKbACeGG/J6iq1VU1XlXjY2NjcyxTkjTdHt2FUlVbgbXAS4HFSfbrNi0FHh5wbZKk3ejnLpSxJIu75QOBk4CN9IL89G63s4Frh1WkJGln+82+C0uANUkW0Qv8q6rquiT3AFck+Vvg28AlQ6xTkjTNrAFeVXcCx83Q/gC98XBJ0gj4JKYkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmN6mdCB0l9WHb+9aMuAYBNF54y6hK0QOyBS1Kj+pkT84gka5Pck2RDkvO69g8keTjJ+u518vDLlSTt0M8QynbgvVV1e5JDgHVJbui2XVxVHxleeZKkXelnTszNwOZu+bEkG4HDh12YJGn39mgMPMkyehMc39o1nZvkziSXJnnmLo5ZlWQiycTk5OS8ipUk/ULfAZ7kYOAa4F1V9SjwSeBoYDm9HvpHZzquqlZX1XhVjY+NjQ2gZEkS9BngSfanF96XVdXnAarqkap6oqqeBD4FrBhemZKk6fq5CyXAJcDGqrpoSvuSKbu9Ebh78OVJknaln7tQTgDOAu5Ksr5rez9wZpLlQAGbgLcNpUJJ0oz6uQvlZiAzbPry4MuRJPXLR+k1L3vD4+M+Oq59lY/SS1KjDHBJapQBLkmNMsAlqVF+iSlpKPaGL7if6uyBS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjepnTswjkqxNck+SDUnO69qfleSGJPd1788cfrmSpB366YFvB95bVccCxwPvSHIscD5wY1UdA9zYrUuSFsisAV5Vm6vq9m75MWAjcDhwKrCm220NcNqwipQk7WyPxsCTLAOOA24FDquqzd2mHwCH7eKYVUkmkkxMTk7Oo1RJ0lR9B3iSg4FrgHdV1aNTt1VVATXTcVW1uqrGq2p8bGxsXsVKkn6hrwBPsj+98L6sqj7fNT+SZEm3fQmwZTglSpJm0s9dKAEuATZW1UVTNn0JOLtbPhu4dvDlSZJ2pZ8p1U4AzgLuSrK+a3s/cCFwVZJzgAeBPxhOiZKkmcwa4FV1M5BdbH7VYMuRJPXLJzElqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUf3MiXlpki1J7p7S9oEkDydZ371OHm6ZkqTp+umBfwZYOUP7xVW1vHt9ebBlSZJmM2uAV9VNwI8XoBZJ0h7oZ1b6XTk3yVuACeC9VfWTmXZKsgpYBXDkkUfO+WTLzr9+zscO0qYLTxl1CZIEzP1LzE8CRwPLgc3AR3e1Y1WtrqrxqhofGxub4+kkSdPNKcCr6pGqeqKqngQ+BawYbFmSpNnMKcCTLJmy+kbg7l3tK0kajlnHwJNcDrwCODTJQ8BfA69IshwoYBPwtiHWKEmawawBXlVnztB8yRBqkSTtAZ/ElKRGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEbNGuBJLk2yJcndU9qeleSGJPd1788cbpmSpOn66YF/Blg5re184MaqOga4sVuXJC2gWQO8qm4Cfjyt+VRgTbe8BjhtwHVJkmYx1zHww6pqc7f8A+CwXe2YZFWSiSQTk5OTczydJGm6eX+JWVUF1G62r66q8aoaHxsbm+/pJEmduQb4I0mWAHTvWwZXkiSpH3MN8C8BZ3fLZwPXDqYcSVK/+rmN8HLgFuAFSR5Kcg5wIXBSkvuAV3frkqQFtN9sO1TVmbvY9KoB1yJJ2gM+iSlJjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNmnVGnt1Jsgl4DHgC2F5V44MoSpI0u3kFeOeVVfXDAXyOJGkPOIQiSY2ab4AX8LUk65KsmmmHJKuSTCSZmJycnOfpJEk7zDfAX1ZVLwFeC7wjycun71BVq6tqvKrGx8bG5nk6SdIO8wrwqnq4e98CfAFYMYiiJEmzm3OAJzkoySE7loHXAHcPqjBJ0u7N5y6Uw4AvJNnxOf9aVV8dSFWSpFnNOcCr6gHgxQOsRZK0B7yNUJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGjWICR32KcvOv37UJQCw6cJTRl2CpBGzBy5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqPmFeBJVib5TpL7k5w/qKIkSbObz6TGi4B/AF4LHAucmeTYQRUmSdq9+fTAVwD3V9UDVfVz4Arg1MGUJUmaTapqbgcmpwMrq+qPuvWzgN+pqnOn7bcKWNWtvgD4zhxrPRT44RyPbZXXvG/wmvcN87nm51TV2PTGof8WSlWtBlbP93OSTFTV+ABKaobXvG/wmvcNw7jm+QyhPAwcMWV9adcmSVoA8wnwbwHHJDkqydOAM4AvDaYsSdJs5jyEUlXbk5wL/DuwCLi0qjYMrLKdzXsYpkFe877Ba943DPya5/wlpiRptHwSU5IaZYBLUqOaCPB97ZH9JJcm2ZLk7lHXshCSHJFkbZJ7kmxIct6oaxq2JAckuS3JHd01f3DUNS2UJIuSfDvJdaOuZSEk2ZTkriTrk0wM9LP39jHw7pH9/wJOAh6id/fLmVV1z0gLG6IkLwe2AZ+tqheNup5hS7IEWFJVtyc5BFgHnPYU/zsOcFBVbUuyP3AzcF5VfXPEpQ1dkvcA48DTq+p1o65n2JJsAsarauAPLrXQA9/nHtmvqpuAH4+6joVSVZur6vZu+TFgI3D4aKsarurZ1q3u37327t7UACRZCpwCfHrUtTwVtBDghwPfm7L+EE/xf9z7siTLgOOAW0dbyfB1QwnrgS3ADVX1lL9m4GPA+4AnR13IAirga0nWdT8tMjAtBLj2EUkOBq4B3lVVj466nmGrqieqajm9p5hXJHlKD5cleR2wparWjbqWBfayqnoJvV9ufUc3RDoQLQS4j+zvA7px4GuAy6rq86OuZyFV1VZgLbBy1LUM2QnAG7ox4SuAE5N8brQlDV9VPdy9bwG+QG9YeCBaCHAf2X+K677QuwTYWFUXjbqehZBkLMnibvlAel/S3zvaqoarqi6oqqVVtYzev+OvV9WbR1zWUCU5qPtiniQHAa8BBnZ32V4f4FW1HdjxyP5G4KohP7I/ckkuB24BXpDkoSTnjLqmITsBOItej2x99zp51EUN2RJgbZI76XVSbqiqfeK2un3MYcDNSe4AbgOur6qvDurD9/rbCCVJM9vre+CSpJkZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalR/w+X75tVRsYTfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(target_p, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 655) 12257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iterative_imputer =  IterativeImputer(random_state=0, estimator=ExtraTreesRegressor(n_estimators=10, random_state=0))\\ndata_p_p = iterative_imputer.fit_transform(data_p)\\nprint (data_p.shape, np.count_nonzero(np.isnan(data_p)))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "simple_imputer =  SimpleImputer(strategy='mean')  # Needs to modify\n",
    "data_p_r = simple_imputer.fit_transform(data_p)\n",
    "print (data_p.shape, np.count_nonzero(np.isnan(data_p)))\n",
    "\n",
    "'''iterative_imputer =  IterativeImputer(random_state=0, estimator=ExtraTreesRegressor(n_estimators=10, random_state=0))\n",
    "data_p_p = iterative_imputer.fit_transform(data_p)\n",
    "print (data_p.shape, np.count_nonzero(np.isnan(data_p)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_p_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6774fc2c1bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata_p_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_p_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_p_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_p_p' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(ratio='not majority', k_neighbors=1)\n",
    "data_p_r, target_p_r = smote.fit_sample(data_p_r, target_p)\n",
    "\n",
    "'''smote = SMOTE(ratio=0.5, k_neighbors=1)\n",
    "data_p_p, target_p_p = smote.fit_sample(data_p_p, target_p)'''\n",
    "\n",
    "\n",
    "#plot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(target_p_p, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "X, y = data_p, target_p\n",
    "print(X.shape)\n",
    "transformer = GenericUnivariateSelect(chi2, 'k_best', param=100) #f_classif, chi2\n",
    "X_new = transformer.fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "\n",
    "\n",
    "# Add the noisy data to the informative features\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(data_p)\n",
    "# X = data_p\n",
    "\n",
    "cols_bool_nonzero = X.max(axis=0) > 0.0\n",
    "print (cols_bool_nonzero.shape)\n",
    "\n",
    "X = X.transpose()[cols_bool_nonzero].transpose()\n",
    "y = target_p\n",
    "print(X.shape)\n",
    "\n",
    "plt.figure(1, figsize=(100, 10))\n",
    "plt.clf()\n",
    "\n",
    "X_indices = np.arange(X.shape[-1])/2.0\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function: the 10% most significant features\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(X, y)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "plt.bar(X_indices + .05, scores, width=.1,\n",
    "        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',\n",
    "        edgecolor='black')\n",
    "\n",
    "# #############################################################################\n",
    "# Compare to the weights of an SVM\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "svm_weights = (clf.coef_ ** 2).sum(axis=0)\n",
    "svm_weights /= svm_weights.max()\n",
    "\n",
    "plt.bar(X_indices + .15, svm_weights, width=.1, label='SVM weight',\n",
    "        color='navy', edgecolor='black')\n",
    "\n",
    "clf_selected = svm.SVC(kernel='linear')\n",
    "clf_selected.fit(selector.transform(X), y)\n",
    "\n",
    "svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\n",
    "svm_weights_selected /= svm_weights_selected.max()\n",
    "\n",
    "plt.bar(X_indices[selector.get_support()] + .25, svm_weights_selected,\n",
    "        width=.1, label='SVM weights after selection', color='c',\n",
    "        edgecolor='black')\n",
    "\n",
    "\n",
    "plt.title(\"Comparing feature selection\")\n",
    "plt.xlabel('Feature number')\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(folder_path + 'feature_selection', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "X, y = data_p, target_p\n",
    "# #############################################################################\n",
    "# Create a feature-selection transform, a scaler and an instance of SVM that we\n",
    "# combine together to have an full-blown estimator\n",
    "clf = Pipeline([('anova', SelectPercentile(chi2)),\n",
    "                ('scaler', Normalizer()),\n",
    "                ('svc', SVC(gamma=\"auto\"))])\n",
    "\n",
    "# #############################################################################\n",
    "# Plot the cross-validation score as a function of percentile of features\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    clf.set_params(anova__percentile=percentile)\n",
    "    this_scores = cross_val_score(clf, X, y, cv=5)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "\n",
    "plt.errorbar(percentiles, score_means, np.array(score_stds))\n",
    "plt.title(\n",
    "    'Performance of the SVM-Anova varying the percentile of features selected')\n",
    "plt.xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.axis('tight')\n",
    "plt.savefig(folder_path + 'svm-anova_norm', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "X, y = data_p, target_p\n",
    "# #############################################################################\n",
    "# Create a feature-selection transform, a scaler and an instance of SVM that we\n",
    "# combine together to have an full-blown estimator\n",
    "clf = Pipeline([('anova', SelectPercentile(chi2)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('svc', SVC(kernel='rbf', gamma=\"auto\"))])\n",
    "\n",
    "# #############################################################################\n",
    "# Plot the cross-validation score as a function of percentile of features\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    clf.set_params(anova__percentile=percentile)\n",
    "    this_scores = cross_val_score(clf, X, y, cv=10)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "\n",
    "plt.errorbar(percentiles, score_means, np.array(score_stds))\n",
    "plt.title(\n",
    "    'Performance of the SVM-Anova varying the percentile of features selected', pad=10)\n",
    "plt.xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.axis('tight')\n",
    "plt.savefig(folder_path + 'svm-anova_std', dpi=120, pad_inch=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run later\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = data_p_p, target_p_p\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(5),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X, y)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.savefig(folder_path + 'RFECV_kerlinear', dpi=120, pad_inch=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p_r.shape, data_p_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross Validation Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "X, y = data_p_r[:,rfecv.support_], target_p_r\n",
    "#X, y = data_p_r, target_p_r\n",
    "print (X.shape, y.shape)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "print (cross_validate(clf, X, y, scoring=['recall_macro', 'precision_macro', 'f1_macro', 'accuracy'], cv=5) )\n",
    "y_pred = cross_val_predict(clf,X, y, cv=5 )\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Feature Selection\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "X = data_p\n",
    "y = target_real_p\n",
    "\n",
    "f_test, _ = f_regression(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "feature_l = pd.read_csv(folder_path + 'dataset_LCPatient_fltrcols.csv').iloc[:,1:].columns.tolist()\n",
    "print(feature_l[0:5])\n",
    "print(len(feature_l))\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25,300))\n",
    "for i in range(X.shape[1]):\n",
    "    plt.subplot(45, 5, i + 1)\n",
    "    plt.scatter(X[:, i], y, edgecolor='black', s=20)\n",
    "    plt.xlabel(\"${}$\".format(feature_l[i]), fontsize=14)\n",
    "    #plt.xlabel(\"$x_{}$\".format(i+1), fontsize=14)\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"$y$\", fontsize=14)\n",
    "    plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]),\n",
    "              fontsize=12)\n",
    "    if(f_test[i]>0.5 or mi[i]>0.5):\n",
    "        print(\"feature={:40.40s} F-test={:.2f}, MI={:.2f}\".format(feature_l[i],f_test[i], mi[i]))\n",
    "plt.savefig(folder_path + 'f_regression, mutual_info_regression', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "X, y = data_p, target_p\n",
    "# #############################################################################\n",
    "# Create a feature-selection transform, a scaler and an instance of SVM that we\n",
    "# combine together to have an full-blown estimator\n",
    "clf = Pipeline([('anova', SelectPercentile(mutual_info_classif)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('svc', SVC(kernel='rbf', gamma=\"auto\"))])\n",
    "\n",
    "# #############################################################################\n",
    "# Plot the cross-validation score as a function of percentile of features\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    clf.set_params(anova__percentile=percentile)\n",
    "    this_scores = cross_val_score(clf, X, y, cv=10)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "\n",
    "plt.errorbar(percentiles, score_means, np.array(score_stds))\n",
    "plt.title(\n",
    "    'Performance of the SVM-Anova varying the percentile of features selected', pad=10)\n",
    "plt.xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.axis('tight')\n",
    "plt.savefig(folder_path + 'svm-anova_mutula_info_classif_std', dpi=120, pad_inch=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Import some data to play with\n",
    "X, y = data_p, target_p\n",
    "# #############################################################################\n",
    "# Create a feature-selection transform, a scaler and an instance of SVM that we\n",
    "# combine together to have an full-blown estimator\n",
    "clf = Pipeline([('anova', SelectPercentile(mutual_info_classif)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('svc', SVC(kernel='rbf', gamma=\"auto\"))])\n",
    "\n",
    "# #############################################################################\n",
    "# Plot the cross-validation score as a function of percentile of features\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    clf.set_params(anova__percentile=percentile)\n",
    "    this_scores = cross_val_score(clf, X, y, cv=10)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "\n",
    "plt.errorbar(percentiles, score_means, np.array(score_stds))\n",
    "plt.title(\n",
    "    'Performance of the SVM-Anova varying the percentile of features selected', pad=10)\n",
    "plt.xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.axis('tight')\n",
    "plt.savefig(folder_path + 'svm-anova_f_classif_std', dpi=120, pad_inch=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "# Need to include it future changes\n",
    "df = pd.read_csv(folder_path + 'dataset_DementiaPatient_fltrcols_allfeatures.csv')\n",
    "df = df.drop(columns=['patient_id'],axis=1)\n",
    "display(df.head(5))\n",
    "\n",
    "# Impute Values\n",
    "simple_imputer =  SimpleImputer(strategy='median')\n",
    "df_array = simple_imputer.fit_transform(df)\n",
    "print (df_array.shape, np.count_nonzero(np.isnan(df_array)))\n",
    "df = pd.DataFrame(data=df_array, columns=df.columns)\n",
    "\n",
    "plt.figure(figsize=(100,100))\n",
    "cor = df.corr()\n",
    "#sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "#plt.show()\n",
    "# Correlation with output variable\n",
    "#cor_target = abs(cor[\"survivalMonths\"])\n",
    "cor_target = abs(cor[\"patient_class\"])\n",
    "\n",
    "# Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target > 0.15]\n",
    "print(relevant_features.sort_values(ascending=False))\n",
    "print(\"{}\".format('-'*80))\n",
    "\n",
    "#cor_target = abs(cor[\"survivalMonthsCat\"])\n",
    "\n",
    "# Selecting highly correlated features\n",
    "#relevant_features = cor_target[cor_target > 0.2]\n",
    "#print(relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection for classification task\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif \n",
    "\n",
    "\n",
    "df = pd.read_csv(folder_path + 'dataset_DementiaPatient_fltrcols_allfeatures.csv')\n",
    "df = df.drop(columns=['patient_id'],axis=1)\n",
    "display(df.head(5))\n",
    "\n",
    "#impute missing values\n",
    "simple_imputer =  SimpleImputer(strategy='median')\n",
    "df_array = simple_imputer.fit_transform(df)\n",
    "print (df_array.shape, np.count_nonzero(np.isnan(df_array)))\n",
    "df = pd.DataFrame(data=df_array, columns=df.columns)\n",
    "\n",
    "\n",
    "X = df.iloc[:,1:]  #independent columns\n",
    "y = df.iloc[:,0:1].values.reshape(df.shape[0])  #target column i.e price range\n",
    "\n",
    "print (X.shape, y.shape, type(X), type(y), y[0:10], X.columns)\n",
    "#print (X[0:1], y[0:5])\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=mutual_info_classif, k=22)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(22,'Score'))  #print 10 best features'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Build a classification task using 3 informative features\n",
    "df = pd.read_csv(folder_path + 'dataset_DementiaPatient_fltrcols_allfeatures.csv')\n",
    "df = df.drop(columns=['patient_id'],axis=1)\n",
    "display(df.head(5))\n",
    "\n",
    "# impute missing values\n",
    "#impute missing values\n",
    "simple_imputer =  SimpleImputer(strategy='median')\n",
    "df_array = simple_imputer.fit_transform(df)\n",
    "print (df_array.shape, np.count_nonzero(np.isnan(df_array)))\n",
    "df = pd.DataFrame(data=df_array, columns=df.columns)\n",
    "\n",
    "\n",
    "\n",
    "fea_list = df.columns[1:].tolist()\n",
    "#print (fea_list)\n",
    "X, y = data_p, target_p\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "#print (indices)\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature: %s (%f)\" % (f + 1, fea_list[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "'''plt.figure(figsize=(219,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\", width=0.1)\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Feature Importance\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build a classification task using 3 informative features\n",
    "df = pd.read_csv(folder_path + 'dataset_DementiaPatient_fltrcols_allfeatures.csv')\n",
    "print (df.info())\n",
    "df = df.drop(columns=['patient_id'],axis=1)\n",
    "display(df.head(5))\n",
    "fea_list = df.columns[1:].tolist()\n",
    "#print(fea_list)\n",
    "#print (fea_list)\n",
    "X, y = data_p, target_p\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = RandomForestClassifier(n_estimators='warn',\n",
    "                              random_state=0, bootstrap=False, criterion='gini',max_depth=5, max_features=100, min_samples_split=10)\n",
    "\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "#print (indices)\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature: %s (%f)\" % (f + 1, fea_list[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "'''plt.figure(figsize=(219,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\", width=0.1)\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
